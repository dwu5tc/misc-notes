ACID
	- atomicity: guarantee all or none of a transation happens. nothing (crash, power failure, error, etc.) allows for a state in which some related changes have happened
	- consitency: consistent data (with valid state [???]); none of the constraints on related data will ever be violated
	- isolation: one transation cannot read data from another that hasn't yet completed. 2 concurrently executing transations see the world as if they were sequentially executed [???]. if one needs to read data written by another, it will have to wait until the other is finished
	durability: once a transation = complete, gurantee all changes have been recorded to a durable medium (e.g hard disk). as well the transaction completion itself is recorded 
	
	e.g
	- A: transation to transfer funds involves withrawal + deposit. if deposit fails, withdrawal should not happen either 
	- C: db tracking a checking account only alows unique check numbers to exist for each transation
	- I: someone checking a balance = isolated from a concurrent transac involving a withdrawal from the same acccount. only when the transac commits successfully and the person checks again will the new balance be reported
	- D: system crash (or other failure) not allowed to lose results of a transac or the contents of the db. often achieved through separated transation logs that can "re-create" all transacs from specific point (like a backup)

introduction
	- rdbms = relational db management sys
	- mongodb = nosql
	- relational dbs use tables/rows 
		- impedance mismatch = misalignment of app layer objs to tables/rows
		- might have to write a mapping layer or use ORM* to translate between the obj in memory and what is saved in the db (troublesome)
	- mongodb uses collections
	- mongo db has no schema. documents in the same collection can be diff
	- scalability
		- relational dbs
			- required relational db engine to manage writes
			- to ensure consistency and atomicity, tables/rows must be locked and only 1 write/access is allowed at a time
			- protecting referential integrity across multiple tables/rows increases lock time
			- inc lock time --> less writes/updates per sec --> higher latency of transactions --> slower app
			- scaling out/replicating data to other servers = worse
				- if relational engines try to enforce consistency and extend locks across the network, lock times are then longer --> transation latency higher --> slower app
			- solution? 
				- denormalization
					- process of inc read performance at the expense of dec write performance by adding  redundant copies of data or by grouping data
					- locks taken, taken only on a single table NOT several
				- relax consistency
					- allow dirty reads**. but might lose durability/consistency
	- mongo's scalability solution 
		- no schema to protect --> faster locking 
		- single document write scope. documents live in a collection but updating documents = one at a time 
			- simpler locking; no need to extend across collections, no relationships to enforce 
		- mongo does not allow for locking across multiple servers 
		- replica set consists of 1 pimary server which accepts all writes and several secondary servers (which will be replicated to); no locks extending from primary to secondaries
		- capped collection = fixed size [???] 
		- eventual consistency: eventually data will propogate from the primary to the secondaries 
		- custom consistency
			- several consistency models ranging from higher latency + absolute consistency to relaxed consistency and some guarantee of durability
			- choice of latter --> returns control to app sooner
			- consistency choices: complete, fire and forget, majority (or specified # of secondaries)
				- fire and forget (limited durability): hand over update cmd to primary, control returned immediately. no guarantee primary doesn't fail (document never makes it to disk --> lost) 
		- inconsistency issues caused by lag of propogating data from primary to secondaries 
		- responsibility shift to the app since no schema enforcement
	*object-relational mapping
	**dirty read: reading a value from another transation which is not committed; chance that the value may rolled back
	
intro to mongodb

replica set
	- with 1 server --> risk of no db during recovery. with replica set --> automatic recovery
	- minimal replica set = 3 (primary + secondary + arbiter)
		- technically only need 1 server (run on diff TCP ports) but in production, should run 1 mongo server per machine
	- primary: one and only writable instance
	- secondary: read only instances. can have many of these --> scalability. data 	repliacted from primary. if primary fails, one of the secondaries takes over
	- arbiter: does not store data. only purpose = break ties during elections
	- [... SKIPPED SECTION ...]
	
	db.getMongo()
	rs.status()
	
	cd foo 
	mkdir \foo\db1 \foo\db2 \foo\db3 
	start "a" mongod --dbpath ./db1 --port 60000 --replSet "demo"
	start "b" mongod --dbpath ./db1 --port 40000 --replSet "demo"
	start "c" mongod --dbpath ./db1 --port 50000 --replSet "demo"
	
	mongo --port 30000
	var demoConfig={_id: "demo", members: [{_id: 0, host: "localhost:60000", priority: 10}, {_id: 1, host: "localhost:40000"}, {_id: 2, host: "localhost:50000", arbiterOnly: true}]};
	rs.initiate(demoConfig)
	
	mongo --port 40000
	db.setSlaveOk()	
	
"C:\Program Files\MongoDB\Server\3.4\bin\mongod.exe" --config "C:\Program Files\MongoDB\Server\3.4\mongod.cfg" --install

the mongo shell
	- shell modes
	- shell invoking syntax
	- using eval
	- substantial scripts
	- excute script before entering
	- shell keys and shortcuts
		ctrl + a = start of line	
		ctrl + e = end of line 
		ctrl + k = delete rest of line
		ctrl + l = clear screen
	- external editor integration
		set EDITOR="//PATH TO THE EDITOR" 
	- load script
		load("foo.js");	
	- user RC file (run script each time enter shell)
		- mongorc.js
		- disabling
			mongo --norc	

saving data
	[ ... ]
	- BSON format
		- advantages: [...]
	- saving documents
		- every document must have _id field 
		- limit to 16mb
	- cannot issue commands across multiple collections
	
	db, show collections
	
	- _id field can be anything EXCEPT an array 
	- ObjectId() function returns diff object ID each time
		- timestamped + [ ... ]
		
		ObjectId().getTimestamp()
		
	- _ids affect reading/writing/sorting and index management [???]
	
	- save vs insert
		- save overwrites documents with same _id
		- insert prevents overwriting (raises duplicate key error)
		- doesn't make sense to use insert without _id field
	
	.find().pretty()
	
	- update
		- atomic within a document
		- update commands issued concurrently will be executed sequentially 
		
		db.foo.update(query, update, options) // options: one?  multi? upsert?
		
		$set, $unset, $inc, $rename, $push/$addToSet, $pull (remove all instances from array), $pop (1, -1),  
	- find and modify	
		db.foo.findAndModify({
			query: <document>, // which document?
			update: <document>, // what change?
			upsert: <boolean>, 
			remove: <boolean>, // delete?
			new: <boolean>, // return the doc pre or post modification?
			sort: <document>, // query order
			fields <document> // return what?
		})
		
- finding documents
	- find
		db.foo.find(query, projection)
			- projection: which fields will be returned; OPTIONAL
		- $lt/gt, $lte/gte, $not, 
		- $in/$nin (include/not include)
			- array format e.g {$inc: [1, 5, 10]}
		- $all
		- dot notation [ ... ]
		- null and $exists 
		- , acts as and &&?
		
		
		- ARRAYS???
		
		- cursor (returns pointer)
			var c = db.foo.find({}, {name: 1})
			
			c.size()
			c.hasNext()
			c.forEach(function(d) {print(d.name)})
			c.sort({name: -1})
			
			db.foo.find({}, {_id: 1}).sort({name: -1})
				- only ids returned (projection) but sorted but name
				- proves sorting = done serverside 
			
	
	
			

	

